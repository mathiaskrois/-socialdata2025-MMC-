{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voted = [\"VoteWatch-EP-voting-data_2004-2022/~$EP6_Voted docs.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP7_Voted docs.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP8_Voted docs.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP9_Voted docs.xlsx\"]\n",
    "rcv = [\"VoteWatch-EP-voting-data_2004-2022/EP6_RCVs_2022_06_13.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP7_RCVs_2014_06_19.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP8_RCVs_2019_06_25.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP9_RCVs_2022_06_22.xlsx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_name(first_name, last_name):\n",
    "    import unicodedata\n",
    "    \n",
    "    if not isinstance(first_name, str):\n",
    "        first_name = str(first_name) if first_name is not None else \"\"\n",
    "    if not isinstance(last_name, str):\n",
    "        last_name = str(last_name) if last_name is not None else \"\"\n",
    "    \n",
    "    first_name = first_name.lower().strip()\n",
    "    last_name = last_name.lower().strip()\n",
    "    \n",
    "    def normalize_chars(text):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "        return text\n",
    "    \n",
    "    first_name = normalize_chars(first_name)\n",
    "    last_name = normalize_chars(last_name)\n",
    "    \n",
    "    for char in ['-', \"'\", \"`\", \".\", \",\", \"&\", \"'\"]:  # Added apostrophe variants\n",
    "        first_name = first_name.replace(char, ' ')\n",
    "        last_name = last_name.replace(char, ' ')\n",
    "    \n",
    "    while '  ' in first_name:\n",
    "        first_name = first_name.replace('  ', ' ')\n",
    "    while '  ' in last_name:\n",
    "        last_name = last_name.replace('  ', ' ')\n",
    "        \n",
    "    first_name = ' '.join(word.capitalize() for word in first_name.split())\n",
    "    last_name = ' '.join(word.capitalize() for word in last_name.split())\n",
    "    \n",
    "    full_name = f\"{first_name} {last_name}\".strip()\n",
    "    \n",
    "    return full_name\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "  \n",
    "    text = text.lower()\n",
    "    \n",
    "    for char in ['&', ',', '-']:\n",
    "        text = text.replace(char, ' ')\n",
    "    \n",
    "    text = text.replace(' and ', ' ')\n",
    "    \n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    \n",
    "    return text.strip()    \n",
    "\n",
    "def process_ep_voting_data(rcv_files, voted_docs_files):\n",
    "\n",
    "    if len(rcv_files) != len(voted_docs_files):\n",
    "        raise ValueError(\"The lists of RCV files and Voted docs files must have the same length\")\n",
    "     \n",
    "    all_data = []\n",
    "    \n",
    "    for i, (rcv_file, voted_doc_file) in enumerate(zip(rcv_files, voted_docs_files)):\n",
    "        print(f\"Processing files {i+1}/{len(rcv_files)}: {rcv_file} and {voted_doc_file}\")\n",
    "        \n",
    "        if \"EP6\" in rcv_file:\n",
    "            ep_session = \"EP6\"\n",
    "            rcv_data = pd.read_excel(rcv_file, header=1)\n",
    "        elif \"EP7\" in rcv_file:\n",
    "            ep_session = \"EP7\"\n",
    "            rcv_data = pd.read_excel(rcv_file, sheet_name=0)\n",
    "        elif \"EP8\" in rcv_file:\n",
    "            ep_session = \"EP8\"\n",
    "            rcv_data = pd.read_excel(rcv_file, sheet_name=0)\n",
    "        elif \"EP9\" in rcv_file:\n",
    "            ep_session = \"EP9\"\n",
    "            rcv_data = pd.read_excel(rcv_file, sheet_name=0)\n",
    "        else:\n",
    "            ep_session = \"Unknown\"\n",
    "            rcv_data = pd.read_excel(rcv_file, sheet_name=0)\n",
    "            print(\"UNKNOWN SESSION\")\n",
    "\n",
    "        rcv_data = rcv_data.dropna(how='all')\n",
    "        \n",
    "        voted_docs = pd.read_excel(voted_doc_file)\n",
    "\n",
    "\n",
    "        # Get vote columns headers (index)\n",
    "        vote_columns = rcv_data.columns[10:].tolist()\n",
    "\n",
    "       \n",
    "        votes_df = process_votes_ep(rcv_data, voted_docs, vote_columns, ep_session=ep_session)\n",
    "\n",
    "        print(f\"Should be total length: {len(vote_columns) + len(voted_docs)}\")\n",
    "      \n",
    "        # Add EP session information\n",
    "        votes_df['ep_session'] = ep_session\n",
    "        \n",
    "        # Append to the list of results\n",
    "        all_data.append(votes_df)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Perform final cleaning\n",
    "    combined_df = clean_combined_data(combined_df)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def process_votes_ep(rcv_data, voted_docs, vote_columns, ep_session = None):\n",
    "    \"\"\"Process voting data for EP7, EP8, EP9 sessions\"\"\"\n",
    "\n",
    "    total_skipped = 0\n",
    "\n",
    "    if ep_session == 'EP6':\n",
    "        date = 'date'\n",
    "        title = 'title'\n",
    "        policy_area = 'main_policy_name'\n",
    "        vote_id_key = 'euro_act_id'\n",
    "        author = 'author_name'\n",
    "\n",
    "        mep_id_key = 'WebisteEpID'\n",
    "\n",
    "    else:\n",
    "        date = 'Date'\n",
    "        title = 'Title'\n",
    "        policy_area = 'De'\n",
    "        vote_id_key = 'Vote ID'\n",
    "        author = 'Author'\n",
    "\n",
    "        mep_id_key = 'WebisteEpID'\n",
    "\n",
    "        if ep_session == 'EP7':\n",
    "            mep_id_key = 'MEP ID'\n",
    "\n",
    "        if ep_session == 'EP8':\n",
    "            policy_area = \"De/Policy area\"\n",
    "\n",
    "        elif ep_session == 'EP9':\n",
    "            policy_area = 'Policy area'\n",
    "  \n",
    "    \n",
    "    # Create a dictionary to map vote IDs to vote information\n",
    "    vote_info = {}\n",
    "    for _, row in voted_docs.iterrows():\n",
    "\n",
    "        vote_info[str(row[vote_id_key])] = {\n",
    "            'date': row[date],\n",
    "            'title': row[title],\n",
    "            'policy_area': row[policy_area],\n",
    "            'author': author,\n",
    "        }\n",
    "    \n",
    "    # Create a list to store results\n",
    "    results = []\n",
    "    \n",
    "    # Process each MEP's votes\n",
    "    for _, mep_row in rcv_data.iterrows():\n",
    "        country = mep_row['Country']\n",
    "        party = mep_row['Party']\n",
    "        epg = mep_row['EPG']\n",
    "\n",
    "        first_name = mep_row['Fname']\n",
    "        last_name = mep_row['Lname']\n",
    "        \n",
    "        mep_id = mep_row[mep_id_key]\n",
    "    \n",
    "        # Process each vote for this MEP\n",
    "        for vote_col in vote_columns:\n",
    "\n",
    "            vote_code = f'{ep_session}-{vote_col}' \n",
    "            \n",
    "            if vote_col not in vote_info:\n",
    "                total_skipped += 1\n",
    "                continue\n",
    "                \n",
    "            mep_vote_code = mep_row[vote_col]\n",
    "            \n",
    "            if mep_vote_code == 0:\n",
    "                continue\n",
    "                \n",
    "            info = vote_info[vote_col]\n",
    "            \n",
    "            results.append({\n",
    "                'full name': clean_name(first_name, last_name),\n",
    "                'country': country,\n",
    "                'national_party': party,\n",
    "                'epg': epg,\n",
    "                'mep_id': mep_id,\n",
    "                'vote_code': vote_code,\n",
    "                'vote': mep_vote_code,\n",
    "                'date': info['date'],\n",
    "                'title': info['title'],\n",
    "                'policy_area': clean_text(info['policy_area']),\n",
    "            })\n",
    "    \n",
    "    print(f\"Were not able to match: {total_skipped} votes\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def clean_combined_data(df):\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['policy_area_cleaned'] = df['policy_area'].str.strip().str.lower()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files 1/3: VoteWatch-EP-voting-data_2004-2022/EP7_RCVs_2014_06_19.xlsx and VoteWatch-EP-voting-data_2004-2022/EP7_Voted docs.xlsx\n",
      "Were not able to match: 5936880 votes\n",
      "Processing files 2/3: VoteWatch-EP-voting-data_2004-2022/EP8_RCVs_2019_06_25.xlsx and VoteWatch-EP-voting-data_2004-2022/EP8_Voted docs.xlsx\n",
      "Were not able to match: 8795358 votes\n",
      "Processing files 3/3: VoteWatch-EP-voting-data_2004-2022/EP9_RCVs_2022_06_22.xlsx and VoteWatch-EP-voting-data_2004-2022/EP9_Voted docs.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markusswegmark/Desktop/Visualization/-socialdata2025-MMC-/.venv/lib/python3.13/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Were not able to match: 10915249 votes\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Visualization/-socialdata2025-MMC-/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m voted_docs_files = voted_docs_files[\u001b[32m1\u001b[39m:]\n\u001b[32m      5\u001b[39m rcv_files = rcv_files[\u001b[32m1\u001b[39m:]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m combined_df = \u001b[43mprocess_ep_voting_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrcv_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoted_docs_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Save the combined dataframe\u001b[39;00m\n\u001b[32m      9\u001b[39m output_file = \u001b[33m\"\u001b[39m\u001b[33mep_voting_data_combined.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mprocess_ep_voting_data\u001b[39m\u001b[34m(rcv_files, voted_docs_files)\u001b[39m\n\u001b[32m     98\u001b[39m combined_df = pd.concat(all_data, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Perform final cleaning\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m combined_df = \u001b[43mclean_combined_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m combined_df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 200\u001b[39m, in \u001b[36mclean_combined_data\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_combined_data\u001b[39m(df):\n\u001b[32m    198\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform final cleaning and transformations on the combined dataframe\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    201\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m].dt.year\n\u001b[32m    202\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m].dt.month\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Visualization/-socialdata2025-MMC-/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Visualization/-socialdata2025-MMC-/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'date'"
     ]
    }
   ],
   "source": [
    "\n",
    "voted_docs_files = [\"VoteWatch-EP-voting-data_2004-2022/EP6_Voted docs.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP7_Voted docs.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP8_Voted docs.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP9_Voted docs.xlsx\"]\n",
    "rcv_files = [\"VoteWatch-EP-voting-data_2004-2022/EP6_RCVs_2022_06_13.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP7_RCVs_2014_06_19.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP8_RCVs_2019_06_25.xlsx\", \"VoteWatch-EP-voting-data_2004-2022/EP9_RCVs_2022_06_22.xlsx\"]\n",
    "\n",
    "voted_docs_files = voted_docs_files[1:]\n",
    "rcv_files = rcv_files[1:]\n",
    "combined_df = process_ep_voting_data(rcv_files, voted_docs_files)\n",
    "\n",
    "# Save the combined dataframe\n",
    "output_file = \"ep_voting_data_combined.csv\"\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column headers:\n",
      "['full name', 'country', 'national_party', 'epg', 'mep_id', 'vote_code', 'vote', 'date', 'title', 'policy_area', 'ep_session', 'year', 'month', 'policy_area_cleaned']\n",
      "\n",
      "Total number of headers: 14\n"
     ]
    }
   ],
   "source": [
    "# Get all column headers as a list\n",
    "headers_list = combined_df.columns.tolist()\n",
    "\n",
    "# Print the list of headers\n",
    "print(\"Column headers:\")\n",
    "print(headers_list)\n",
    "\n",
    "# Print the total number of headers\n",
    "print(f\"\\nTotal number of headers: {len(headers_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325599, 14)\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
